# Crawler Service

High-performance web crawler built with Golang and Colly framework.

## Features

- ğŸš€ Fast concurrent crawling
- ğŸ”„ Redis-based job queue
- ğŸ“Š HTML parsing and data extraction
- ğŸ¯ Configurable crawl depth and limits
- ğŸ“ Structured logging

## Development

```bash
# Install dependencies
go mod download

# Run locally
go run cmd/worker/main.go

# Build
go build -o bin/crawler-worker ./cmd/worker

# Run tests
go test ./...
```

## Configuration

Set environment variables in `.env`:

- `REDIS_URL`: Redis connection string
- `DB_HOST`, `DB_PORT`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`: Database configuration
- `LOG_LEVEL`: Logging level (debug, info, warn, error)

## Architecture

```
crawler-service/
â”œâ”€â”€ cmd/worker/        - Entry point for crawler worker
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ engine/        - Core crawler logic using Colly
â”‚   â”œâ”€â”€ parser/        - HTML parsing utilities
â”‚   â””â”€â”€ queue/         - Redis queue implementation
â””â”€â”€ pkg/               - Shared packages (config, logger)
```
